version: "3.9"

# manual execution
# docker run --rm -p 9091:8080 -p 7077:7077 -p 4747:4040 spark-test-master
# docker run --rm -p 9093:8081 -p 4848:4040 spark-test-worker spark://spark-master:7077
# to check files
# docker run -it --rm -p 9091:8080 -p 7077:7077 -p 4747:4040 spark-test-master bash

services:
  spark-master:
    container_name: spark-master
    image: spark-test-master
    #build:
    #  context: .
    #  dockerfile: master/Dockerfile
    ports:
      - "9090:8080" # Master Web UI
      - "7077:7077" # Master listen port
      - "4747:4040" # Spark Driver Web UI (for client mode)
      - "5005:5005" # for intellij remote debug
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk. Spark uses temporary scratch space to spill data to disk during shuffles and other operations
      - ../data/master/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/master/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net
  spark-worker-a:
    container_name: spark-worker-a
    image: spark-test-worker
    #build:
    #  context: .
    #  dockerfile: worker/Dockerfile
    ports:
      - "9091:8081" # Worker Web UI
      - "4848:4040" # Spark Driver Web UI. Your application's dashboard, which shows memory and workload data. Can be swap out through spark.ui.port
    depends_on:
      - spark-master
    command: ["spark://spark-master:7077"]
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_WORKER_CORES=10 # Total number of cores to allow Spark applications to use on the machine (default: all available cores).
      - SPARK_WORKER_MEMORY=10G # Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); note that each application's individual memory is configured using its spark.executor.memory property.
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk. Spark uses temporary scratch space to spill data to disk during shuffles and other operations
      - ../data/worker-a/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/worker-a/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net

  spark-worker-b:
    container_name: spark-worker-b
    image: spark-test-worker
    ports:
      - "9092:8081" # Worker Web UI
      - "4949:4040" # Spark Driver Web UI. Your application's dashboard, which shows memory and workload data. Can be swap out through spark.ui.port
    depends_on:
      - spark-master
    command: ["spark://spark-master:7077"]
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_WORKER_CORES=10 # Total number of cores to allow Spark applications to use on the machine (default: all available cores).
      - SPARK_WORKER_MEMORY=10G # Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); note that each application's individual memory is configured using its spark.executor.memory property.
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk. Spark uses temporary scratch space to spill data to disk during shuffles and other operations
      - ../data/worker-b/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/worker-b/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net

networks:
  dev_spark_net:
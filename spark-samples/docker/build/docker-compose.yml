version: "3.9"

# manual execution
# docker run --rm -p 9091:8080 -p 7077:7077 -p 4747:4040 spark-test-master
# docker run --rm -p 9093:8081 -p 4848:4040 spark-test-worker spark://spark-master:7077
# to check files
# docker run -it --rm -p 9091:8080 -p 7077:7077 -p 4747:4040 spark-test-master bash

services:
  spark-master:
    container_name: spark-master
    image: spark-test-master
    #build:
    #  context: .
    #  dockerfile: master/Dockerfile
    ports:
      - "9090:8080" # Master Web UI
      - "7077:7077" # Master listen port
      - "4747:4040" # Spark Driver Web UI (for client mode)
      - "5005:5005" # for intellij remote debug
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk.
      - SPARK_LOCAL_DIRS=/opt/spark/scratch
      # For standalone cluster, Directory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).
      - SPARK_WORKER_DIR=/opt/spark/work
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Default directory to use for "scratch" space in Spark, overridden by SPARK_LOCAL_DIRS above in environment. Added this volume to see if something will be saved there.
      - ../data/master/tmp:/tmp
      # Directory to use for "scratch" in our standalone cluster (see SPARK_LOCAL_DIRS env above)
      - ../data/master/scratch:/opt/spark/scratch
      # For standalone cluster, to see files in the Directory to run applications in
      - ../data/master/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/master/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net
  spark-worker-a:
    container_name: spark-worker-a
    image: spark-test-worker
    #build:
    #  context: .
    #  dockerfile: worker/Dockerfile
    ports:
      - "9091:8081" # Worker Web UI
      - "4848:4040" # Spark Driver Web UI. Your application's dashboard, which shows memory and workload data. Can be swap out through spark.ui.port
    depends_on:
      - spark-master
    command: ["spark://spark-master:7077"]
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_WORKER_CORES=10 # Total number of cores to allow Spark applications to use on the machine (default: all available cores).
      - SPARK_WORKER_MEMORY=10G # Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); note that each application's individual memory is configured using its spark.executor.memory property.
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk.
      - SPARK_LOCAL_DIRS=/opt/spark/scratch
      # For standalone cluster, Directory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).
      - SPARK_WORKER_DIR=/opt/spark/work
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Default directory to use for "scratch" space in Spark, overridden by SPARK_LOCAL_DIRS above in environment. Added this volume to see if something will be saved there.
      - ../data/worker-a/tmp:/tmp
      # Directory to use for "scratch" in our standalone cluster (see SPARK_LOCAL_DIRS env above)
      - ../data/worker-a/scratch:/opt/spark/scratch
      # For standalone cluster, to see files in the Directory to run applications in
      - ../data/worker-a/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/worker-a/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net

  spark-worker-b:
    container_name: spark-worker-b
    image: spark-test-worker
    ports:
      - "9092:8081" # Worker Web UI
      - "4949:4040" # Spark Driver Web UI. Your application's dashboard, which shows memory and workload data. Can be swap out through spark.ui.port
    depends_on:
      - spark-master
    command: ["spark://spark-master:7077"]
    environment:
      # https://spark.apache.org/docs/latest/spark-standalone.html
      - SPARK_WORKER_CORES=10 # Total number of cores to allow Spark applications to use on the machine (default: all available cores).
      - SPARK_WORKER_MEMORY=10G # Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GiB); note that each application's individual memory is configured using its spark.executor.memory property.
      - SPARK_DAEMON_MEMORY=1G # Memory to allocate to the Spark master and worker daemons themselves (default: 1g).
      # Directory to use for "scratch" space in Spark, including map output files and RDDs that get stored on disk.
      - SPARK_LOCAL_DIRS=/opt/spark/scratch
      # For standalone cluster, Directory to run applications in, which will include both logs and scratch space (default: SPARK_HOME/work).
      - SPARK_WORKER_DIR=/opt/spark/work
    volumes:
      ############ Custom user directories
      - ../apps:/opt/spark-apps
      - ../data:/opt/spark-data
      ############ Spark default directories
      # Default directory to use for "scratch" space in Spark, overridden by SPARK_LOCAL_DIRS above in environment. Added this volume to see if something will be saved there.
      - ../data/worker-b/tmp:/tmp
      # Directory to use for "scratch" in our standalone cluster (see SPARK_LOCAL_DIRS env above)
      - ../data/worker-b/scratch:/opt/spark/scratch
      # For standalone cluster, to see files in the Directory to run applications in
      - ../data/worker-b/work:/opt/spark/work
      # The default location for managed databases and tables. Default (value of $PWD/spark-warehouse) - config: spark.sql.warehouse.dir
      - ../data/worker-b/spark-warehouse:/opt/spark/spark-warehouse
    networks:
      - dev_spark_net

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop-hive.env
    ports:
      - "50070:50070"
    networks:
      - dev_spark_net
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    volumes:
      - datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop-hive.env
    environment:
      SERVICE_PRECONDITION: "namenode:50070"
    ports:
      - "50075:50075"
    networks:
      - dev_spark_net

networks:
  # network (not docker0) for this dockercompose services (change subnet if you have network conflict)
  dev_spark_net:
    ipam:
      config:
        - subnet: 182.28.0.0/16
volumes:
  namenode:
  datanode: